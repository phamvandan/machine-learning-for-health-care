{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supersaiyan/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torchvision.datasets as datasets # Has standard datasets we can import in a nice way\n",
    "import torchvision.transforms as transforms # Transformations we can perform on our dataset\n",
    "import torch.nn.functional as F # All functions that don't have any parameters\n",
    "from torch.utils.data import DataLoader, Dataset # Gives easier dataset managment and creates mini batches\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dataset = ImageFolder(\"base_dir/train_dir/\")\n",
    "train_data, test_data, train_label, test_label = train_test_split(dataset.imgs, dataset.targets, test_size=0.2, random_state=42)\n",
    "# dataset = ImageFolder(\"base_dir/val_dir/\")\n",
    "# _, test_data, _, test_label = train_test_split(dataset.imgs, dataset.targets, test_size=1., random_state=42)\n",
    "\n",
    "# ImageLoader Class\n",
    "\n",
    "class ImageLoader(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = self.checkChannel(dataset) # some images are CMYK, Grayscale, check only RGB \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.dataset[item][0])\n",
    "        classCategory = self.dataset[item][1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, classCategory\n",
    "        \n",
    "    \n",
    "    def checkChannel(self, dataset):\n",
    "        datasetRGB = []\n",
    "        for index in range(len(dataset)):\n",
    "            if (Image.open(dataset[index][0]).getbands() == (\"R\", \"G\", \"B\")): # Check Channels\n",
    "                datasetRGB.append(dataset[index])\n",
    "        return datasetRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(\"base_dir/val_dir/\")\n",
    "eval1_data, eval2_data, eval1_label, eval2_label = train_test_split(dataset.imgs, dataset.targets, test_size=0.5, random_state=42)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), transforms.ToTensor(), # transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "eval1_dataset = ImageLoader(eval1_data, test_transform)\n",
    "eval2_dataset = ImageLoader(eval2_data, test_transform)\n",
    "eval1_loader = DataLoader(eval1_dataset, batch_size=16, shuffle=True)\n",
    "eval2_loader = DataLoader(eval2_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), transforms.RandomHorizontalFlip(), transforms.ToTensor(),# transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "]) # train transform\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), transforms.ToTensor(), # transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "]) # test transform\n",
    "\n",
    "train_dataset = ImageLoader(train_data, train_transform)\n",
    "test_dataset = ImageLoader(test_data, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "from collections import OrderedDict\n",
    "from timm.models.layers.activations import *\n",
    "import timm\n",
    "# load pretrain model and modify...\n",
    "# model = models.EfficientNet(pretrained=True)\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True, drop_rate=0.2)\n",
    "\n",
    "# If you want to do finetuning then set requires_grad = False\n",
    "# Remove these two lines if you want to train entire model,\n",
    "# and only want to load the pretrain weights.\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# # num_ftrs = model.fc.in_features\n",
    "# fc = nn.Sequential(OrderedDict([\n",
    "# \t\t\t\t\t\t\t\t#  ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "# \t\t\t\t\t\t\t    #  ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "# \t\t\t\t\t\t\t\t#  ('dropout1', nn.Dropout(0.7)),\n",
    "#                                  ('fc2', nn.Linear(1280, 512)),\n",
    "# \t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "# \t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "# \t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "# \t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "# \t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "# \t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "# \t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 3)),\n",
    "# \t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "# \t\t\t\t\t\t\t ]))\n",
    "# \n",
    "\n",
    "model.classifier = nn.Sequential(model.classifier, nn.Linear(1000, 512), nn.Linear(512, 7), nn.Softmax(dim=1))\n",
    "for param in model.classifier.parameters():\n",
    "\tparam.requires_grad = True\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supersaiyan/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/supersaiyan/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (fc4): Linear(in_features=2048, out_features=7, bias=True)\n",
       "    (output): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "from collections import OrderedDict\n",
    "from timm.models.layers.activations import *\n",
    "import timm\n",
    "# load pretrain model and modify...\n",
    "# model = models.EfficientNet(pretrained=True)\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# If you want to do finetuning then set requires_grad = False\n",
    "# Remove these two lines if you want to train entire model,\n",
    "# and only want to load the pretrain weights.\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "fc = nn.Sequential(OrderedDict([#('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "# \t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "# \t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "#                                  ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t#  ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t#  ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t#  ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t#  ('fc3', nn.Linear(num_ftrs, 128)),\n",
    "\t\t\t\t\t\t\t\t#  ('BN3', nn.BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t    #  ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(num_ftrs, 7)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "\n",
    "\n",
    "model.fc = fc\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "optimizer.zero_grad()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x)\n",
    "            _, predictions = torch.max(output, 1)\n",
    "            correct += (predictions == y).sum().item()\n",
    "            test_loss = criterion(output, y)\n",
    "            \n",
    "    test_loss /= len(loader.dataset)\n",
    "    print(\"Average Loss: \", test_loss, \"  Accuracy: \", correct, \" / \",\n",
    "    len(loader.dataset), \"  \", int(correct / len(loader.dataset) * 100), \"%\")\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test\n",
    "\n",
    "def train(num_epoch, model, patience = 10):\n",
    "    best_acc = 0\n",
    "    patience_count = 0\n",
    "    \n",
    "    for epoch in range(0, num_epoch):\n",
    "#         current_loss = 0.0\n",
    "#         current_corrects = 0\n",
    "        losses = []\n",
    "        model.train()\n",
    "        loop = tqdm(enumerate(train_loader), total=len(train_loader)) # create a progress bar\n",
    "        for batch_idx, (data, targets) in loop:\n",
    "            if len(data) != 16:\n",
    "                break\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            scores = model(data)\n",
    "            \n",
    "            loss = criterion(scores, targets)\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, preds = torch.max(scores, 1)\n",
    "#             current_loss += loss.item() * data.size(0)\n",
    "#             current_corrects += (preds == targets).sum().item()\n",
    "#             accuracy = int(current_corrects / len(train_loader.dataset) * 100)\n",
    "            loop.set_description(f\"Epoch {epoch+1}/{num_epoch} process: {int((batch_idx / len(train_loader)) * 100)}\")\n",
    "            loop.set_postfix(loss=loss.data.item())\n",
    "        \n",
    "        test_acc = test()\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            print('NEW BEST=', best_acc)\n",
    "            patience_count = 0\n",
    "            # save model\n",
    "            torch.save({ \n",
    "                        'model_state_dict': model.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict(), \n",
    "                        }, 'bestmodel.pt')\n",
    "        else:\n",
    "            patience_count += 1\n",
    "            if patience_count == patience:\n",
    "                print('EARLY STOPPING, BEST ACC=', best_acc)\n",
    "                break\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 process: 99: 100%|█████████▉| 1928/1929 [03:42<00:00,  8.68it/s, loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  5734  /  7714    74 %\n",
      "NEW BEST= 0.7433238268084003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 process: 99: 100%|█████████▉| 1928/1929 [03:38<00:00,  8.84it/s, loss=1.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6068  /  7714    78 %\n",
      "NEW BEST= 0.7866217267306197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 process: 99: 100%|█████████▉| 1928/1929 [03:42<00:00,  8.66it/s, loss=1.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  5977  /  7714    77 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.98it/s, loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6404  /  7714    83 %\n",
      "NEW BEST= 0.8301788955146487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.25it/s, loss=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6482  /  7714    84 %\n",
      "NEW BEST= 0.8402903811252269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.24it/s, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0003, device='cuda:0')   Accuracy:  6480  /  7714    84 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.23it/s, loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6508  /  7714    84 %\n",
      "NEW BEST= 0.8436608763287529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.24it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6713  /  7714    87 %\n",
      "NEW BEST= 0.8702359346642469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.24it/s, loss=1.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6684  /  7714    86 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.23it/s, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6791  /  7714    88 %\n",
      "NEW BEST= 0.880347420274825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.23it/s, loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0003, device='cuda:0')   Accuracy:  6702  /  7714    86 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.23it/s, loss=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6439  /  7714    83 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200 process: 99: 100%|█████████▉| 1928/1929 [03:29<00:00,  9.22it/s, loss=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6798  /  7714    88 %\n",
      "NEW BEST= 0.8812548612911589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.23it/s, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6902  /  7714    89 %\n",
      "NEW BEST= 0.8947368421052632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 process: 99: 100%|█████████▉| 1928/1929 [03:29<00:00,  9.22it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6887  /  7714    89 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.23it/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6878  /  7714    89 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.23it/s, loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6747  /  7714    87 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/200 process: 99: 100%|█████████▉| 1928/1929 [03:28<00:00,  9.23it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7026  /  7714    91 %\n",
      "NEW BEST= 0.9108115115374643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/200 process: 99: 100%|█████████▉| 1928/1929 [03:29<00:00,  9.22it/s, loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6917  /  7714    89 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 process: 99: 100%|█████████▉| 1928/1929 [03:29<00:00,  9.22it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7039  /  7714    91 %\n",
      "NEW BEST= 0.9124967591392273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6918  /  7714    89 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  9.00it/s, loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7064  /  7714    91 %\n",
      "NEW BEST= 0.9157376199118485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6977  /  7714    90 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  9.00it/s, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6965  /  7714    90 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7071  /  7714    91 %\n",
      "NEW BEST= 0.9166450609281825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  9.00it/s, loss=1.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  6835  /  7714    88 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7066  /  7714    91 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7185  /  7714    93 %\n",
      "NEW BEST= 0.9314233860513352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7264  /  7714    94 %\n",
      "NEW BEST= 0.9416645060928183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  9.00it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7306  /  7714    94 %\n",
      "NEW BEST= 0.9471091521908219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7265  /  7714    94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  9.00it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7269  /  7714    94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7262  /  7714    94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7321  /  7714    94 %\n",
      "NEW BEST= 0.9490536686543946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7348  /  7714    95 %\n",
      "NEW BEST= 0.9525537982888255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7306  /  7714    94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7339  /  7714    95 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7289  /  7714    94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/200 process: 99: 100%|█████████▉| 1928/1929 [03:35<00:00,  8.96it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7291  /  7714    94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7271  /  7714    94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7320  /  7714    94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7278  /  7714    94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7320  /  7714    94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7320  /  7714    94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/200 process: 99: 100%|█████████▉| 1928/1929 [03:34<00:00,  8.99it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7345  /  7714    95 %\n",
      "EARLY STOPPING, BEST ACC= 0.9525537982888255\n"
     ]
    }
   ],
   "source": [
    "train(200, model) # train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaludate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0002, device='cuda:0')   Accuracy:  7345  /  7714    95 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.952164894996111"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x)\n",
    "            _, predictions = torch.max(output, 1)\n",
    "            correct += (predictions == y).sum().item()\n",
    "            test_loss = criterion(output, y)\n",
    "            \n",
    "    test_loss /= len(loader.dataset)\n",
    "    print(\"Average Loss: \", test_loss, \"  Accuracy: \", correct, \" / \",\n",
    "    len(loader.dataset), \"  \", int(correct / len(loader.dataset) * 100), \"%\")\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0025, device='cuda:0')   Accuracy:  438  /  469    93 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9339019189765458"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(eval1_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.0025, device='cuda:0')   Accuracy:  433  /  469    92 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9232409381663113"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(eval2_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restnet_50 valid: 0.952 test: 0.925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supersaiyan/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/supersaiyan/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "from collections import OrderedDict\n",
    "from timm.models.layers.activations import *\n",
    "import timm\n",
    "model = models.resnet50(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "fc = nn.Sequential(OrderedDict([\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(num_ftrs, 7)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "model.fc = fc\n",
    "print('Built model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Loading checkpoint\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"----> Loading checkpoint\")\n",
    "checkpoint = torch.load(\"./restnet50_92_5.pt\") # Try to load last checkpoint\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "model.to(device)\n",
    "model.eval()\n",
    "print('loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomImagePrediction(filepath):\n",
    "    img_array = Image.open(filepath).convert(\"RGB\")\n",
    "    data_transforms=transforms.Compose([\n",
    "        transforms.Resize((224, 224)), \n",
    "        transforms.ToTensor(), \n",
    "        # transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    img = data_transforms(img_array).unsqueeze(dim=0) # Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "    load = DataLoader(img)\n",
    "    \n",
    "    for x in load:\n",
    "        x=x.to(device)\n",
    "        pred = model(x)[0].cpu().detach().numpy()\n",
    "        print(pred)\n",
    "        return {\n",
    "            'akiec':pred[0], \n",
    "            'bcc': pred[1], \n",
    "            'bkl': pred[2], \n",
    "            'df': pred[3],\n",
    "            'mel': pred[4],\n",
    "            'nv': pred[5],\n",
    "            'vasc': pred[6]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9970610e-31 3.5728524e-24 8.2722826e-24 2.2845349e-26 5.0844127e-21\n",
      " 1.0000000e+00 1.5727463e-23]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'akiec': 1.997061e-31,\n",
       " 'bcc': 3.5728524e-24,\n",
       " 'bkl': 8.2722826e-24,\n",
       " 'df': 2.2845349e-26,\n",
       " 'mel': 5.0844127e-21,\n",
       " 'nv': 1.0,\n",
       " 'vasc': 1.5727463e-23}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomImagePrediction(\"./base_dir/val_dir/nv/ISIC_0024327.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
